---
title: "The Road to Diffusion Models: Score-based Models"
date: 2024-07-15T20:26:12+08:00
type : list-single
author: Xiwei Pan
slug: diffusion-model-score
draft: false
toc: true
categories:
  - learning
tags:
  - diffusion models
  - knowledge acquisition
---
## Score-based Generative Models
**Put In Front:** Several core ingredients regarding score-based generative models --- the Langevin equation, the (Stein) score function, and the score-matching loss.

[This blog](https://xiweipan.com/en/2024/07/12/diffusion-model-vdm/) have shown that we can actually train a VDM by optimizing a neural network (NN) `$\pmb{s}_{\pmb{\theta}}(\pmb{x}_t, t)$` to predict the score function `$\nabla log\,p(\pmb{x}_t)$`. The score term in that derivation arises directly from Tweedie's Formula, which offers limited insight into the nature of the score function or the reasons for modeling it. Therefore, we resort to another class of generative models, <font color=Crimson>Score-based Generative Models</font>[^1][^2], for some interpretations. And from the perspective of the results, the previously deived VDM can be shown to have an equivalent Score-based Generative Modeling formulation, which allows us to flexibly switch between these two interpretations.

Instead of directly starting from showcasing "why score function?", we first revisit the **energy-based models**[^3][^4]. Arbitrarily flexible probability distributions can be expressed by:
`$$p_{\pmb{\theta}}(\pmb{x})=\frac{1}{Z_{\pmb{\theta}}}e^{-f_{\pmb{\theta}}(\pmb{x})}, \tag{1} \label{eq1}$$`
where `$f_{\pmb{\theta}}(\pmb{x})$` denotes an arbitrary flexible, parameterizable function. It is called the **energy function** and often modeled by a NN. `$Z_{\pmb{\theta}}$` is a *normalizing constant* to ensure `$\int p_{\pmb{\theta}}(\pmb{x})\,\mathrm{d}\pmb{x}=1$`. Maximum likelihood poses a possible way to learn such a distribution; <font color=Crimson>however, this requires tractably computing the normalizing constant `$Z_{\pmb{\theta}}=\int e^{-f_{\pmb{\theta}}(\pmb{x})}\,\mathrm{d}\pmb{x}$`, which may not be accessible for complex `$f_{\pmb{\theta}}(\pmb{x})$` functions.</font>

Training a NN `$\pmb{s}_{\pmb{\theta}}(\pmb{x})$` to learn the score function `$\nabla log\,p(\pmb{x}_t)$` of distribution `$p(\pmb{x})$` turns out to be a way out of calculating or modeling `$Z_{\pmb{\theta}}$`. To arrive at this conclusion, we simply take the derivative of the log of both sides of Equation `$\eqref{eq1}$`:
`\begin{align}
\nabla_{\pmb{x}} log\,p_{\pmb{\theta}}(\pmb{x}_t) &= \nabla_{\pmb{x}} log\left(\frac{1}{Z_{\pmb{\theta}}}e^{-f_{\pmb{\theta}}(\pmb{x})}\right) \tag{2}\\
&= \nabla_{\pmb{x}} log\frac{1}{Z_{\pmb{\theta}}}+\nabla_{\pmb{x}} log\,e^{-f_{\pmb{\theta}}(\pmb{x})} \tag{3}\\
&= -\nabla_{\pmb{x}} f_{\pmb{\theta}}(\pmb{x}) \tag{4}\\
&\approx \pmb{s}_{\pmb{\theta}}(\pmb{s}). \tag{5}
\end{align}`
Note that, this is represented as a NN without involving any normalization constants. And this score model can be optimized by minimizing the *Fisher Divergence* with the ground truth score function:
`$$\mathbb{E}_{p(\pmb{x})}\left[\|\pmb{s}_{\pmb{\theta}}(\pmb{x})-\nabla log\,p(\pmb{x})\|_2^2\right] \tag{6} \label{eq6}$$`

The specific explanation is that, for every `$\pmb{x}$`, <font color=RoyalBlue>taking the gradient of its log likelihood w.r.t. `$\pmb{x}$` points out the exact direction in data space to move in order to further increase the likelihood.</font> As is illustrated in the figure (Fig. 1) below.
{{<figure src="/figures/blogFigs/diffusionModel/diffusion_fig5.png" caption="Figure 1: Visualization of three random sampling trajectories generated with Langevin dynamics, all starting from the same initialization point, for a Mixture of Gaussians. The left figure plots these sampling trajectories on a 3D contour, while the right figure plots sampling trajectories against the ground truth score function." width="800">}}

From the same initialization point, we can generate samples from different modes due to the *stochastic noise term* in the **Langevin dynamics** sampling procedure; without it, sampling from a fixed point would always deterministically follow the score to the same mode at every trial. Score function defines a vector field over the entire space that data `$\pmb{x}$` inhabits, pointing towards the modes (as shown in the right panel of Fig. 1).

### Why Langevin Equation?
Suppose that if we are given `$p(\pmb{x})$`, we should aim to draw samples from a location where `$p(\pmb{x})$` has a high value. Thus an optimization scheme can be naturally obtained from the idea of searching for a higher probability
`$$\pmb{x}^\ast=\mathop{\arg\max}\limits_{\pmb{x}} log\,p(\pmb{x}),$$`
and the goal is to maximize the log-likelihood of the distribution `$p(\pmb{x})$`. Before going deeper into Langevin equation, we first leave a comment about the difference between the maximization here and maximum likelihood estimation. <font color=Crimson>In the former scheme, we have fixed model parameters and a changing data point; while in the latter one, the data point `$\pmb{x}$` is fixed but the model parameters are changing.</font> Intuitively, we have the following table to summarize the difference.

| Problem | **Sampling** | **Maximum Likelihood** |
|:---------:|:---------:|:---------:|
| Optimization target | A sample `$\pmb{x}$` | Model parameter `$\pmb{\theta}$` |
| Formulation | `$\pmb{x}^\ast=\mathop{\arg\max}\limits_{\pmb{x}} log\,p(\pmb{x}; \pmb{\theta})$` | `$\pmb{\theta}^\ast=\mathop{\arg\max}\limits_{\pmb{\theta}} log\,p(\pmb{x}; \pmb{\theta})$` |


[^1]: Yang Song and Stefano Ermon. [Generative modeling by estimating gradients of the data distribution](https://arxiv.org/pdf/1907.05600). Advances in Neural Information Processing Systems, 32, 2019.
[^2]: Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint [arXiv:2011.13456](https://arxiv.org/pdf/2011.13456), 2020.
[^3]: Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.
[^4]: Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint [arXiv:2101.03288](https://arxiv.org/pdf/2101.03288), 2021.