---
title: "The Road to Diffusion Models: Score-based Models"
date: 2024-07-15T20:26:12+08:00
type : list-single
author: Xiwei Pan
slug: diffusion-model-score
draft: false
toc: true
categories:
  - learning
tags:
  - diffusion models
  - knowledge acquisition
---
**To Start With:** Several core ingredients regarding score-based generative models --- the Langevin equation, the (Stein's) score function, and the score-matching loss.

## Langevin Equation: From the Perspective of Sampling
Suppose that if we are given `$p(\pmb{x})$`, we should aim to draw samples from a location where `$p(\pmb{x})$` has a high value. Thus an optimization scheme can be naturally obtained from the idea of searching for a higher probability
`$$\pmb{x}^\ast=\mathop{\arg\max}\limits_{\pmb{x}} log\,p(\pmb{x}),$$`
and the goal is to maximize the log-likelihood of the distribution `$p(\pmb{x})$`. Before going deeper into Langevin equation, we first leave a comment about the difference between the maximization here and maximum likelihood estimation. <font color=RoyalBlue>In the former scheme, we have fixed model parameters and a changing data point; while in the latter one, the data point `$\pmb{x}$` is fixed but the model parameters are changing.</font> Intuitively, we have the following table to summarize the difference.

| Problem | **Sampling** | **Maximum Likelihood** |
|:---------:|:---------:|:---------:|
| Optimization target | A sample point `$\pmb{x}$` | Model parameters `$\pmb{\theta}$` |
| Formulation | `$\pmb{x}^\ast=\mathop{\arg\max}\limits_{\pmb{x}} log\,p(\pmb{x}; \pmb{\theta})$` | `$\pmb{\theta}^\ast=\mathop{\arg\max}\limits_{\pmb{\theta}} log\,p(\pmb{x}; \pmb{\theta})$` |

Instead of being a simple parameteric model, which can be studied by analytic solutions, optimizations of `$p(\pmb{x})$` in a high-dimensional space are generally ill-posed with many local minima. Therefore, there exists no single algorithm that is globally converging in all situations. <font color=Crimson>A reasonable trade-off between *computational complexity, memeory requirement, difficulty of implementation, and solution quality* is the **gradient descent algorithm**, a first-order method.</font> For optimization with the objective function being `$log\,p(\pmb{x})$`, we have an update rule defined by the gradient descent algorithm
`$$\pmb{x}_{t+1}=\pmb{x}_t+\tau\nabla_{\pmb{x}}log\,p(\pmb{x}_t),$$`
where `$\tau$` is the step size which users can control.

We wonâ€™t go into detail about the mathematical derivation of Langevin dynamics here (refer to [this blog](https://xiweipan.com/en/2024/10/15/langevin-dynamics/) for details), but will simply treat it as an iterative process that allows us to draw samples. Here, we present the corresponding definition.

> The (discrete-time) **Langevin equation** for sampling from a known distribution `$p(\pmb{x})$` is an iterative precodure for `$t=1,\cdots,T$`:
> `$$\pmb{x}_{t+1}=\pmb{x}_t+\tau\nabla_{\pmb{x}}log\,p(\pmb{x}_t)+\sqrt{2\tau}\pmb{\epsilon},\quad \pmb{\epsilon}\sim\mathcal{N}\left(\pmb{\epsilon}; \pmb{0}, \pmb{\mathrm{I}}\right), \tag{1} \label{eq1}$$`
> where `$\pmb{x}_0$` is the white noise, which is randomly sampled from a prior distribution (such as uniform). `$\pmb{\epsilon}$` is an extra noise term to ensure that the generated samples do not always collapse onto a mode, but hover around it for diversity.

Note that, in generative models, `Langevin equation = gradient descent + noise` is applied to the log-likelihood function. One question arises: **why do we need an extra "noise" instead of just "gradient descent"?**

Here is one interpretation: <font color=Crimson>The problem we are interested in is *sampling* from a distribution instead of the optimization.</font> With the introduction of a random noise to the original gradient descent step, we can randomly pick a sample that follows the trajectory of the objective function while not collapsing onto a deterministic mode. If we are closer to the peak, we will move left and right slightly; if we are far from the peak, the gradient direction will pull us towards the peak. In a word, by repeatedly initialize the algorithm at a uniformly distributed location, we will eventually collect samples that will follow the designated distribution.

## Langevin Equation: From the Perspective of Energy-based Models
Starting from the energy-based model serves as another path to arrive at the score function. [This blog](https://xiweipan.com/en/2024/07/12/diffusion-model-vdm/) have shown that we can actually train a VDM by optimizing a neural network (NN) `$\pmb{s}_{\pmb{\theta}}(\pmb{x}_t, t)$` to predict the score function `$\nabla log\,p(\pmb{x}_t)$`. The score term in that derivation arises directly from Tweedie's Formula, which offers limited insight into the nature of the score function or the reasons for modeling it. Therefore, we resort to another class of generative models, <font color=Crimson>Score-based Generative Models</font>[^1][^2], for some interpretations. And from the perspective of the results, the previously derived VDM can be shown to have an equivalent Score-based Generative Modeling formulation, which allows us to flexibly switch between these two interpretations.

Instead of directly starting from showcasing "why score function?", we first revisit the **energy-based models**[^3][^4]. Arbitrarily flexible probability distributions can be expressed by:
`$$p_{\pmb{\theta}}(\pmb{x})=\frac{1}{Z_{\pmb{\theta}}}e^{-f_{\pmb{\theta}}(\pmb{x})}, \tag{2} \label{eq2}$$`
where `$f_{\pmb{\theta}}(\pmb{x})$` denotes an arbitrary flexible, parameterizable function. It is called the **energy function** and often modeled by a NN. `$Z_{\pmb{\theta}}$` is a *normalizing constant* to ensure `$\int p_{\pmb{\theta}}(\pmb{x})\,\mathrm{d}\pmb{x}=1$`. Maximum likelihood poses a possible way to learn such a distribution; <font color=Crimson>however, this requires tractably computing the normalizing constant `$Z_{\pmb{\theta}}=\int e^{-f_{\pmb{\theta}}(\pmb{x})}\,\mathrm{d}\pmb{x}$`, which may not be accessible for complex `$f_{\pmb{\theta}}(\pmb{x})$` functions.</font>

Training a NN `$\pmb{s}_{\pmb{\theta}}(\pmb{x})$` to learn the score function `$\nabla log\,p(\pmb{x}_t)$` of distribution `$p(\pmb{x})$` turns out to be a way out of calculating or modeling `$Z_{\pmb{\theta}}$`. To arrive at this conclusion, we simply take the derivative of the log of both sides of Equation `$\eqref{eq2}$`:
`\begin{align}
\nabla_{\pmb{x}} log\,p_{\pmb{\theta}}(\pmb{x}_t) &= \nabla_{\pmb{x}} log\left(\frac{1}{Z_{\pmb{\theta}}}e^{-f_{\pmb{\theta}}(\pmb{x})}\right) \tag{3}\\
&= \nabla_{\pmb{x}} log\frac{1}{Z_{\pmb{\theta}}}+\nabla_{\pmb{x}} log\,e^{-f_{\pmb{\theta}}(\pmb{x})} \tag{4}\\
&= -\nabla_{\pmb{x}} f_{\pmb{\theta}}(\pmb{x}) \tag{5}\\
&\approx \pmb{s}_{\pmb{\theta}}(\pmb{x}). \tag{6}
\end{align}`
Note that, this is represented as a NN without involving any normalization constants. The specific explanation for the above derivation is that, for every `$\pmb{x}$`, <font color=RoyalBlue>taking the gradient of its log likelihood w.r.t. `$\pmb{x}$` points out the exact direction in data space to move in order to further increase the likelihood.</font> As is illustrated in the left panel of Fig. 1.
{{<figure src="/figures/blogFigs/diffusionModel/diffusion_fig5.png" caption="Figure 1: Visualization of three random sampling trajectories generated with Langevin dynamics, all starting from the same initialization point, for a Mixture of Gaussians. The left figure plots these sampling trajectories on a 3D contour, while the right figure plots sampling trajectories against the ground truth score function (Calvin Luo, 2022)." width="800">}}

The score function defines a vector field over the entire space, pointing towards the modes (as shown in the right panel of Fig. 1). So, by learning the score of the true data distribution, samples can be generated by starting at any arbitrary point in the same space and then iteratively following the direction to finally reach a mode. This sampling procedure is known as **Langevin dynamics** defined by Equation `$\eqref{eq1}$`.

Also, from the same initialization point, we can generate samples from different modes due to the *stochastic noise term* `$\sqrt{2\tau}\pmb{\epsilon}$` in the Langevin equation; without it, sampling from a fixed point would always deterministically follow the score to the same mode at every trial. This is particularly useful when sampling is initialized from a position that lies between multiple modes.

## (Stein's) Score Function
The second term on the right side of Langevin dynamics equation (Equation `$\eqref{eq1}$`) is known as the **Stein's score function**, which is denoted by
`$$\pmb{s}_{\pmb{\theta}}(\pmb{x})\overset{\mathrm{def}}{=}\nabla_{\pmb{x}}log\,p_{\pmb{\theta}}(\pmb{x}), \tag{7} \label{eq7}$$`
notice that, the word "Stein's" is added here to distinguish it from the **ordinary score function**, which takes the form
`$$\pmb{s}_{\pmb{x}}(\pmb{\theta})\overset{\mathrm{def}}{=}\nabla_{\pmb{\theta}}log\,p_{\pmb{\theta}}(\pmb{x}).$$`
Maximum likelihood estimation uses the ordinary score function, whereas Langevin dynamics uses Stein's score function.

The key to understand the score function is to remember that it is the gradient w.r.t. the data `$\pmb{x}$`. So, for any high-dimensional distribution `$p(\pmb{x})$`, the gradient yields a vector field
`$$\nabla_{\pmb{x}}log\,p_{\pmb{\theta}}(\pmb{x})=\left[\frac{\partial log\,p(\pmb{x})}{\partial x}, \frac{\partial log\,p(\pmb{x})}{\partial y}\right]^\mathrm{T}.$$`

### Geometric Interpretations of the Score Function
Take **Fig. 1** as an illustration.
- The magnitude of vectors corresponds to that of `$\nabla_{\pmb{x}}log\,p(\pmb{x})$`. So, when `$log\,p(\pmb{x})$` approaches the peak, the gradient will be very weak.
- The vector field indicates *how* a data point should travel in the contour map. Specifically, a data point will be dragged all the way towards the basin (where we have the weakest gradient) by the Langevin dynamics.
- Physically, the score function is equivalent to the "*drift*". <font color=Crimson>This suggests how the diffusion particles should flow to the lowest energy state.</font>

## Score Matching Techniques
<font color=RoyalBlue>We should realize that we generally have no access to the ground truth `$p(\pmb{x})$` in the Langevin equation.</font> Fortunately, there are several cheap and dirty ways to approximate the ground truth score function.

<mark>**Explicit Score Matching (ESM).**</mark> Given a dataset `$\chi=\left\lbrace\pmb{x}^{(1)},\cdots,\pmb{x}^{(M)}\right\rbrace$`, the solution one may came up with is to consider the classical kernal density estimation by defining a distribution
`$$q_h(\pmb{x})=\frac{1}{M}\sum_{m=1}^M\frac{1}{h}K\left(\frac{\pmb{x}-\pmb{x}^{(m)}{h}\right), \tag{8} \label{eq8}$$`
where `$h$` is a certain *hyperparameter* for the kernal function `$K( \cdot )$`, and `$\pmb{x}^{(m)}$` represents the `$m$`-th sample in the training set.

Equation `$\eqref{eq8}$` indicates that the sum of all individual kernals yields the overall **kernal density estimate** `$q(\pmb{x})$`. Therefore, `$q(\pmb{x})$` is at best an approximation to the unknown true data distribution `$p(\pmb{x})$`. <font color=RoyalBlue>And we can learn `$\pmb{s}_{\pmb{\theta}}$` based on `$q(\pmb{x})$` as a compromise.</font> This leads to the following definition of the loss function which can be used to train a NN.
> The **explicit score matching** loss is
> `\begin{align}
J_{\mathrm{ESM}}(\pmb{\theta})&\overset{\mathrm{def}}{=}\mathbb{E}_{p(\pmb{x})}\left[\|\pmb{s}_{\pmb{\theta}}(\pmb{x})-\nabla_{\pmb{x}}log\,p(\pmb{x})\|^2\right]\\
&\approx \mathbb{E}_{q_h(\pmb{x})}\left[\|\pmb{s}_{\pmb{\theta}}(\pmb{x})-\nabla_{\pmb{x}}log\,q_h(\pmb{x})\|^2\right]. \tag{9} \label{eq9}
\end{align}`

Substituting Equation `$\eqref{eq8}$` into Equation `$\eqref{eq9}$`, the loss is finally derived by
`\begin{align}
J_{\mathrm{ESM}}(\pmb{\theta})&\overset{\mathrm{def}}{=}\mathbb{E}_{p(\pmb{x})}\left[\|\pmb{s}_{\pmb{\theta}}(\pmb{x})-\nabla_{\pmb{x}}log\,p(\pmb{x})\|^2\right]\\
&\approx \int\|\pmb{s}_{\pmb{\theta}}(\pmb{x})-\nabla_{\pmb{x}}log\,q_h(\pmb{x})\|^2q_h(\pmb{x})\,\mathrm{d}\pmb{x}\\
&= \frac{1}{M}\sum_{m=1}^M\int\frac{1}{h}\|\pmb{s}_{\pmb{\theta}}(\pmb{x})-\nabla_{\pmb{x}}log\,q_h(\pmb{x})\|^2K\left(\frac{\pmb{x}-\pmb{x}^{(m)}}{h}\right)\,\mathrm{d}\pmb{x}.
\end{align}`

The derived loss can be used to train the network `$\pmb{s}_{\pmb{\theta}}$`. Once trained, we can replace the score term in the Langevin equation to obtain the recursion:
`$$\pmb{x}_{t+1}=\pmb{x}_t+\tau\pmb{s}_{\pmb{\theta}}(\pmb{x}_t)+\sqrt{2\tau}\pmb{\epsilon}.$$`

- <font color=Tomato>Drawback regarding this approximation is that the kernal density estimation `$q(\pmb{x})$` is a fairly poor non-parameter estimation of the ground truth `$p(\pmb{x})$`.</font> In particular, if *the number of samples is limited and the samples live in a high dimensional space*, this approximation performance turns out to be poor.

<mark>**Implicit Score Matching (ISM).**</mark> In this method, the original ESM loss (Equation `$\eqref{eq9}$`) is changed to an implicit one
`$$J_{\mathrm{ISM}}(\pmb{\theta})\overset{\mathrm{def}}{=}\mathbb{E}_{p(\pmb{x})}\left[\mathrm{Tr}(\nabla_{\pmb{x}}\pmb{s}_{\pmb{\theta}}(\pmb{x}))+\frac{1}{2}\|\pmb{s}_{\pmb{\theta}}(\pmb{x}))\|^2\right], \tag{10} \label{eq10}$$`
here `$\nabla_{\pmb{x}}\pmb{s}_{\pmb{\theta}}(\pmb{x}))$` is the Jacobian of `$\pmb{s}_{\pmb{\theta}}(\pmb{x}))$`. The ISM loss (Equation `$\eqref{eq10}$`) can be approximated by **Monte Carlo**
`$$J_{\mathrm{ISM}}(\pmb{\theta})\approx\frac{1}{M}\sum_{m=1}^M\sum_{i}\left(\partial_i\pmb{s}_{\pmb{\theta}}(\pmb{x}^{(m)})+\frac{1}{2}\left|\left[\pmb{s}_{\pmb{\theta}}(\pmb{x}^{(m)})\right]_i\right|^2\right),$$`
where `$\partial_i\pmb{s}_{\pmb{\theta}}(\pmb{x}^{(m)})=\frac{\partial}{\partial x_i}\left[\pmb{s}_{\pmb{\theta}}(\pmb{x})\right]_i=\frac{\partial^2}{\partial x_i^2}log\,p(\pmb{x})$`. If a deep NN is used to model the score function, the computation of trace operator can be overwhelming. Therefore, the ISM is not scalable.

<mark>**Denoising Score Matching (DSM).**</mark>

[^1]: Yang Song and Stefano Ermon. [Generative modeling by estimating gradients of the data distribution](https://arxiv.org/pdf/1907.05600). Advances in Neural Information Processing Systems, 32, 2019.
[^2]: Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint [arXiv:2011.13456](https://arxiv.org/pdf/2011.13456), 2020.
[^3]: Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.
[^4]: Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint [arXiv:2101.03288](https://arxiv.org/pdf/2101.03288), 2021.